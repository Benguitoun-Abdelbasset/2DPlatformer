{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b9f4df8-6ba3-467f-871d-ff27e179f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77431462-3053-4846-89a0-f40faf79cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tmx_level(tmx_file_path, map_shape=(1, 15, 50)):\n",
    "    \"\"\"\n",
    "    Loads a level from a .tmx file (in XML format with CSV encoding) into a PyTorch tensor.\n",
    "\n",
    "    Args:\n",
    "    - tmx_file_path (str): Path to the .tmx file\n",
    "    - map_shape (tuple): Shape of the tile map (channels, height, width)\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Tensor of shape (1, 15, 50) or similar based on map_shape\n",
    "    \"\"\"\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(tmx_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Find the data field in the XML structure\n",
    "    data_field = root.find(\".//layer/data[@encoding='csv']\")\n",
    "    if data_field is None:\n",
    "        raise ValueError(\"No CSV-encoded tile data found in this .tmx file.\")\n",
    "\n",
    "    # Get the CSV string of tile values\n",
    "    csv_data = data_field.text.strip()\n",
    "\n",
    "    # Convert the CSV string into a list of integers\n",
    "    tile_values = list(map(int, csv_data.split(',')))\n",
    "\n",
    "    # Reshape the tile values into the map shape (height, width)\n",
    "    height, width = map_shape[1], map_shape[2]\n",
    "    if len(tile_values) != height * width:\n",
    "        raise ValueError(f\"Tile data does not match the expected map size: {height}x{width}.\")\n",
    "    \n",
    "    # Convert to a numpy array and then to a PyTorch tensor\n",
    "    level = np.array(tile_values, dtype=np.int32).reshape((height, width))\n",
    "    level_tensor = torch.tensor(level, dtype=torch.long).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "    return level_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a565e19a-a166-41ec-b195-1d5c8d8c9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_levels_from_folder(folder_path, map_shape=(1, 15, 50)):\n",
    "    \"\"\"\n",
    "    Loads all levels from a folder containing .tmx files into a list of PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder containing .tmx files\n",
    "    - map_shape (tuple): Shape of the tile map (channels, height, width)\n",
    "\n",
    "    Returns:\n",
    "    - list of torch.Tensor: List of tensors, each representing a level\n",
    "    \"\"\"\n",
    "    # List all .tmx files in the directory\n",
    "    tmx_files = [f for f in os.listdir(folder_path) if f.endswith('.tmx')]\n",
    "    \n",
    "    # List to store all levels\n",
    "    all_levels = []\n",
    "\n",
    "    # Iterate through each .tmx file and load it\n",
    "    for tmx_file in tmx_files:\n",
    "        tmx_file_path = os.path.join(folder_path, tmx_file)\n",
    "        try:\n",
    "            # Load the level data from each .tmx file\n",
    "            level_tensor = load_tmx_level(tmx_file_path, map_shape)\n",
    "            all_levels.append(level_tensor)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {tmx_file}: {e}\")\n",
    "\n",
    "    return all_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a758948-479f-489f-91ff-a2c63769d642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1303 levels.\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "folder_path = \"./2D game Project/good/\"\n",
    "levels = load_all_levels_from_folder(folder_path, map_shape=(1, 15, 50))\n",
    "\n",
    "# Check how many levels were loaded\n",
    "print(f\"Loaded {len(levels)} levels.\")\n",
    "\n",
    "\n",
    "float_levels = [level.float() for level in levels]\n",
    "\n",
    "stacked_levels = torch.stack(float_levels, dim=0)\n",
    "\n",
    "levels=stacked_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43ce2f-ba86-4b8a-a845-050e51d8a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DISPLAY ==========\n",
    "\n",
    "def plot_level(level, title=\"Generated Level\"):\n",
    "    level_2d = torch.tensor(level).reshape(15, 50)\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.imshow(level_2d, cmap=\"viridis\", aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b39bcb-c9c5-43d8-bb61-5594b23ea968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AKAI\\AppData\\Local\\Temp\\ipykernel_14468\\2750154168.py:99: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Scalar.cpp:23.)\n",
      "  total_loss += loss.item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Loss: 0.3191\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_level' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m     plot_level(generate_level(model))\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# ========== GENERATION FUNCTION ==========\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_level\u001b[39m(model, start_tile\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m750\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_level' is not defined"
     ]
    }
   ],
   "source": [
    "# Level Transformer: Training to Generate 2D Levels\n",
    "# ================================================\n",
    "# Install any missing libraries\n",
    "# !pip install torch torchvision\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ========== DATA ==========\n",
    "\n",
    "# Example data\n",
    "# Each level is a sequence of 50x15 = 750 tiles\n",
    "\n",
    "\n",
    "# Simulate a dataset: 500 levels\n",
    "dataset = []\n",
    "for level in levels:\n",
    "    flat = [int(i) for i in level.flatten().tolist()]\n",
    "    dataset.append(flat)\n",
    "print(dataset[0])\n",
    "\n",
    "# Add your real levels to `dataset` instead of randoms\n",
    "\n",
    "# Build vocabulary (tile IDs)\n",
    "tile_vocab = list(set([tile for level in dataset for tile in level]))\n",
    "print(tile_vocab)\n",
    "tile_vocab_size = max(tile_vocab) + 1  # assume IDs start from 0 or 1\n",
    "\n",
    "print(tile_vocab_size)\n",
    "# ========== DATASET CLASS ==========\n",
    "\n",
    "class LevelDataset(Dataset):\n",
    "    def __init__(self, levels):\n",
    "        self.levels = levels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.levels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        level = torch.tensor(self.levels[idx], dtype=torch.long)\n",
    "        return level[:-1], level[1:]  # input sequence, target sequence\n",
    "\n",
    "train_dataset = LevelDataset(dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ========== MODEL ==========\n",
    "\n",
    "class LevelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=8, num_layers=4):\n",
    "        super(LevelTransformer, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(1000, embed_dim)  # Positions up to 1000 tokens\n",
    "\n",
    "        transformer_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=512,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(transformer_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        token_embeddings = self.token_emb(x)\n",
    "        positions = torch.arange(0, T, device=x.device).unsqueeze(0)\n",
    "        pos_embeddings = self.pos_emb(positions)\n",
    "        x_emb = token_embeddings + pos_embeddings\n",
    "\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device)\n",
    "        out = self.transformer(x_emb.transpose(0, 1), x_emb.transpose(0, 1), tgt_mask=tgt_mask)\n",
    "        out = self.fc_out(out.transpose(0, 1))\n",
    "        return out\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LevelTransformer(vocab_size=tile_vocab_size).to(device)\n",
    "\n",
    "# ========== TRAINING ==========\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, tile_vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    plot_level(generate_level(model))\n",
    "\n",
    "# ========== GENERATION FUNCTION ==========\n",
    "\n",
    "def generate_level(model, start_tile=1, max_length=750):\n",
    "    model.eval()\n",
    "    generated = [start_tile]\n",
    "    for _ in range(max_length-1):\n",
    "        x = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            next_token_logits = logits[0, -1]\n",
    "            probs = nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated.append(next_token.item())\n",
    "    return generated\n",
    "\n",
    "# ========== GENERATE LEVELS ==========\n",
    "\n",
    "# Generate 3 example levels\n",
    "generated_levels = []\n",
    "for _ in range(3):\n",
    "    new_level = generate_level(model)\n",
    "    print(new_level)\n",
    "    generated_levels.append(new_level)\n",
    "\n",
    "\n",
    "for idx, level in enumerate(generated_levels):\n",
    "    plot_level(level, title=f\"Generated Level {idx+1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0c5af-048c-4e15-b828-dba047f36b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b3d4e-4bc1-4d35-b2b1-991d4b5158bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
